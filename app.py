import os
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import streamlit as st
import google.generativeai as genai
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv

load_dotenv()
os.getenv("GOOGLE_API_KEY")
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# read all pdf files and return text


def get_pdf_text(pdf_docs):
    text = ""
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text

# split text into chunks


def get_text_chunks(text):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=10000, chunk_overlap=1000)
    chunks = splitter.split_text(text)
    return chunks  # list of strings

# get embeddings for each chunk


def get_vector_store(chunks):
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001")  # type: ignore
    vector_store = FAISS.from_texts(chunks, embedding=embeddings)
    vector_store.save_local("faiss_index")


def get_conversational_chain():
    prompt_template = """
    Answer the question as detailed as possible from the provided context, make sure to provide all the details. Include links and references where appropriate.
    If you are unsure about the correct answer, add a note at the end stating that you are unsure about the correct answer, but still provide one.\n\n
    Context:\n {context}?\n
    Question: \n{question}\n

    Answer:
    """

    model = ChatGoogleGenerativeAI(model="gemini-pro",
                                   client=genai,
                                   temperature=0.3,
                                   )
    prompt = PromptTemplate(template=prompt_template,
                            input_variables=["context", "question"])
    print(prompt)
    chain = load_qa_chain(llm=model, chain_type="stuff", prompt=prompt)
    return chain


def clear_chat_history():
    st.session_state.messages = [
        {"role": "assistant", "content": "upload some pdfs and ask me a question"}]


def user_input(user_question):
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001")  # type: ignore

    new_db = FAISS.load_local("faiss_index", embeddings)
    docs = new_db.similarity_search(user_question)

    chain = get_conversational_chain()

    response = chain(
        {"input_documents": docs, "question": user_question}, return_only_outputs=True, )

    print(response)
    return response


def initialise_with_pdf(filename="display.pdf"):
    pdf_docs = [filename]
    raw_text = get_pdf_text(pdf_docs)
    text_chunks = get_text_chunks(raw_text)
    get_vector_store(text_chunks)


def main():
    st.set_page_config(
        page_title="Gemini PDF Chatbot",
        page_icon="ðŸ¤–"
    )

    init_pdf_file = 'initial.pdf'

    # if the directory faiss_index is older than the file display.pdf
    if os.path.exists("faiss_index"):
        if os.path.getmtime("faiss_index/index.pkl") < os.path.getmtime(init_pdf_file):
            initialise_with_pdf(init_pdf_file)
    else:
        initialise_with_pdf(init_pdf_file)
    st.success("Ready")

    with st.sidebar:
        st.title("Menu:")
        st.markdown("proudly served to you by [L-CAS](https://lcas.lincoln.ac.uk/)")
        st.markdown("[![Logo](https://i0.wp.com/lcas.lincoln.ac.uk/wp/wp-content/uploads/2012/05/cropped-lcas_logo_150dpi-720x987.png?&h=100)](https://lcas.lincoln.ac.uk/)")
        st.write("This chat bot uses the [Gemini Pro model](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/) to answer questions about the CMP3103 module. It's conversation context is defined by the content of the PDF file provided below.")
        st.write("__DISCLAIMER:__ *The responses in this chat bot are generated by a generative artificial intelligence large language model. Please note that the information provided may by completely incorrect. You cannot rely upon the provided answers and we accept no liability if you get incorrect answers or responses. You may use this chat bot to explore the content, but you must not rely on it for any assessments. We do not assume any responsibility or liability for the use or interpretation of this content.*")

        with open(init_pdf_file, "rb") as file:
            file.seek(0)
            st.download_button(
                label="Download context PDF file",
                data=file,
                file_name='context.pdf',
                mime="application/pdf",
                key="initial-pdf"
            )    

    # # Sidebar for uploading PDF files
    # with st.sidebar:
    #     st.title("Menu:")
    #     pdf_docs = st.file_uploader(
    #         "Upload your PDF Files and Click on the Submit & Process Button", accept_multiple_files=True)
    #     if st.button("Submit & Process"):
    #         with st.spinner("Processing..."):
    #             raw_text = get_pdf_text(pdf_docs)
    #             text_chunks = get_text_chunks(raw_text)
    #             get_vector_store(text_chunks)
    #             st.success("Done")

    # Main content area for displaying chat messages
    st.title("Chat about CMP3103 content using GeminiðŸ¤–")
    st.write("Welcome to the chat!")
    st.sidebar.button('Clear Chat History', on_click=clear_chat_history)
    st.sidebar.write("*This service is provided 'as is' for students enrolled in the module CMP3103 at the University of Lincoln. It is not intended for any other use.*")

    # Chat input
    # Placeholder for chat messages

    if "messages" not in st.session_state.keys():
        st.session_state.messages = [
            {"role": "assistant", "content": "Talk to me about the CMP3103 module"}]

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])

    if prompt := st.chat_input():
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

    # Display chat messages and bot response
    if st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = user_input(prompt)
                placeholder = st.empty()
                full_response = ''
                for item in response['output_text']:
                    full_response += item
                    placeholder.markdown(full_response)
                placeholder.markdown(full_response)
        if response is not None:
            message = {"role": "assistant", "content": full_response}
            st.session_state.messages.append(message)


if __name__ == "__main__":
    main()
